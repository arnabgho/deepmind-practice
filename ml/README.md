# Machine Learning Interview Preparation - DeepMind

A comprehensive curriculum covering **calculus**, **statistics**, and **deep learning fundamentals** for Google DeepMind ML interviews.

---

## Progress Tracker

**Total Progress: 0/20 topics completed**

- **Phase 1**: 0/5 completed
- **Phase 2**: 0/7 completed
- **Phase 3**: 0/5 completed
- **Phase 4**: 0/3 completed

---

## Phase 1: Mathematical Foundations (Topics 1-5)

### 01. Derivatives & Gradients
**Status**: ⬜ Not Started
**Concepts**: Derivatives, partial derivatives, gradients, directional derivatives
**Why Important**: Foundation for understanding how neural networks learn
**Key Skills**: Computing gradients, understanding gradient descent geometrically

### 02. Chain Rule & Backpropagation Mathematics
**Status**: ⬜ Not Started
**Concepts**: Chain rule, computational graphs, gradient flow
**Why Important**: Core mathematical principle behind backpropagation
**Key Skills**: Manual backprop calculations, understanding gradient chains

### 03. Partial Derivatives & Jacobians
**Status**: ⬜ Not Started
**Concepts**: Multivariable calculus, Jacobian matrices, Hessians
**Why Important**: Understanding parameter spaces and second-order optimization
**Key Skills**: Computing Jacobians, understanding curvature

### 04. Probability & Statistics Fundamentals
**Status**: ⬜ Not Started
**Concepts**: Distributions, expectation, variance, covariance, Bayes' theorem
**Why Important**: Foundation for understanding ML as statistical learning
**Key Skills**: Working with distributions, computing expectations

### 05. Maximum Likelihood Estimation
**Status**: ⬜ Not Started
**Concepts**: MLE, log-likelihood, cross-entropy connection
**Why Important**: Theoretical foundation for loss functions
**Key Skills**: Deriving MLE solutions, connecting to neural network training

---

## Phase 2: Deep Learning Fundamentals (Topics 6-12)

### 06. Neural Network Forward Pass
**Status**: ⬜ Not Started
**Concepts**: Linear transformations, affine layers, matrix operations
**Why Important**: Understanding what happens during prediction
**Key Skills**: Implementing forward pass from scratch, understanding dimensions

### 07. Backpropagation Algorithm
**Status**: ⬜ Not Started
**Concepts**: Gradient computation, backward pass, computational efficiency
**Why Important**: How neural networks actually learn
**Key Skills**: Implementing backprop from scratch, debugging gradient issues

### 08. Activation Functions & Their Derivatives
**Status**: ⬜ Not Started
**Concepts**: ReLU, sigmoid, tanh, softmax; derivatives and properties
**Why Important**: Non-linearity and gradient flow
**Key Skills**: Choosing appropriate activations, understanding gradient vanishing/explosion

### 09. Loss Functions
**Status**: ⬜ Not Started
**Concepts**: MSE, cross-entropy, their derivatives and use cases
**Why Important**: Defining what "good" means for your model
**Key Skills**: Implementing losses, understanding when to use which

### 10. Optimization Algorithms
**Status**: ⬜ Not Started
**Concepts**: SGD, momentum, RMSprop, Adam; learning rate schedules
**Why Important**: How models converge during training
**Key Skills**: Implementing optimizers, tuning learning rates

### 11. Weight Initialization
**Status**: ⬜ Not Started
**Concepts**: Xavier/Glorot, He initialization, impact on training
**Why Important**: Preventing gradient vanishing/explosion from the start
**Key Skills**: Understanding variance preservation, implementing initialization schemes

### 12. Regularization Techniques
**Status**: ⬜ Not Started
**Concepts**: L1/L2 regularization, dropout, early stopping
**Why Important**: Preventing overfitting
**Key Skills**: Implementing regularization, understanding bias-variance tradeoff

---

## Phase 3: Advanced Deep Learning Concepts (Topics 13-17)

### 13. Batch Normalization & Layer Normalization
**Status**: ⬜ Not Started
**Concepts**: Normalization layers, training vs inference mode, backprop through BN
**Why Important**: Stable training and faster convergence
**Key Skills**: Implementing normalization, understanding mode differences

### 14. Convolutional Neural Networks - Mathematics
**Status**: ⬜ Not Started
**Concepts**: Convolution operation, backprop through conv layers, parameter sharing
**Why Important**: Understanding spatial hierarchies and parameter efficiency
**Key Skills**: Computing conv output dimensions, implementing conv backprop

### 15. Recurrent Neural Networks & Gradient Flow
**Status**: ⬜ Not Started
**Concepts**: RNN equations, BPTT, vanishing/exploding gradients, LSTM/GRU
**Why Important**: Understanding sequential data processing
**Key Skills**: Implementing RNN forward/backward pass, understanding gradient flow over time

### 16. Attention Mechanisms
**Status**: ⬜ Not Started
**Concepts**: Attention scores, softmax attention, key-query-value paradigm
**Why Important**: Foundation for modern architectures
**Key Skills**: Implementing scaled dot-product attention, understanding attention weights

### 17. Transformer Architecture
**Status**: ⬜ Not Started
**Concepts**: Self-attention, multi-head attention, positional encoding
**Why Important**: State-of-the-art architecture for many tasks
**Key Skills**: Understanding full transformer forward pass, attention mechanisms

---

## Phase 4: Practical Machine Learning (Topics 18-20)

### 18. Bias-Variance Tradeoff
**Status**: ⬜ Not Started
**Concepts**: Underfitting, overfitting, model complexity
**Why Important**: Diagnosing and fixing model performance issues
**Key Skills**: Identifying bias vs variance problems, choosing appropriate solutions

### 19. Evaluation Metrics & Cross-Validation
**Status**: ⬜ Not Started
**Concepts**: Accuracy, precision, recall, F1, ROC-AUC; k-fold CV
**Why Important**: Properly measuring model performance
**Key Skills**: Choosing appropriate metrics, implementing CV

### 20. Model Debugging & Hyperparameter Tuning
**Status**: ⬜ Not Started
**Concepts**: Learning curves, gradient checking, hyperparameter search strategies
**Why Important**: Practical skills for getting models to work
**Key Skills**: Debugging training issues, systematic hyperparameter tuning

---

## Key Concepts Coverage Map

### Calculus
- **Derivatives**: Topics 1, 2, 8, 9
- **Chain Rule**: Topics 2, 7
- **Partial Derivatives**: Topics 1, 3
- **Gradients**: Topics 1, 2, 7
- **Jacobians**: Topic 3

### Statistics & Probability
- **Probability Distributions**: Topic 4
- **Expectation & Variance**: Topics 4, 18
- **Maximum Likelihood**: Topic 5
- **Bayes' Theorem**: Topic 4

### Deep Learning Core
- **Forward Pass**: Topic 6
- **Backpropagation**: Topics 2, 7, 13, 14, 15
- **Gradient Descent**: Topics 1, 10
- **Activation Functions**: Topic 8
- **Loss Functions**: Topics 5, 9
- **Optimization**: Topic 10

### Deep Learning Advanced
- **Weight Initialization**: Topic 11
- **Regularization**: Topic 12
- **Normalization**: Topic 13
- **CNNs**: Topic 14
- **RNNs**: Topic 15
- **Attention**: Topics 16, 17
- **Transformers**: Topic 17

### Practical Skills
- **Model Evaluation**: Topics 18, 19
- **Debugging**: Topic 20
- **Hyperparameter Tuning**: Topic 20

---

## Study Recommendations

### 4-Week Intensive Plan

**Week 1: Mathematical Foundations**
- Complete Topics 1-5
- Focus on understanding, not just memorization
- Work through all derivations by hand

**Week 2: Deep Learning Basics**
- Complete Topics 6-12
- Implement everything from scratch in NumPy
- Connect theory to code

**Week 3: Advanced Concepts**
- Complete Topics 13-17
- Focus on modern architectures
- Understand attention mechanisms deeply

**Week 4: Practical & Review**
- Complete Topics 18-20
- Review all previous topics
- Practice explaining concepts out loud

### 6-Week Comprehensive Plan

**Weeks 1-2: Phase 1 (2-3 topics per week)**
**Weeks 3-4: Phase 2 (3-4 topics per week)**
**Week 5: Phase 3 (5 topics)**
**Week 6: Phase 4 + Review**

---

## What You'll Be Able To Do

After completing this curriculum:

✓ Explain gradient descent and backpropagation from first principles
✓ Derive loss function gradients mathematically
✓ Implement neural networks from scratch (no frameworks)
✓ Understand why specific techniques work (initialization, normalization, etc.)
✓ Debug training issues (vanishing gradients, overfitting, etc.)
✓ Explain modern architectures (CNNs, RNNs, Transformers)
✓ Answer "why" questions about deep learning choices
✓ Discuss tradeoffs between different approaches

---

## Interview Question Types Covered

### Conceptual Questions
- "Explain backpropagation"
- "Why do we use ReLU instead of sigmoid?"
- "What is batch normalization and why does it work?"
- "Explain the attention mechanism"

### Mathematical Questions
- "Derive the gradient of cross-entropy loss"
- "Show the backprop equations for a simple network"
- "Compute the Jacobian of this transformation"

### Practical Questions
- "How would you debug vanishing gradients?"
- "When would you use L1 vs L2 regularization?"
- "How do you choose a learning rate?"

### Implementation Questions
- "Implement softmax and its gradient"
- "Write the forward pass for a conv layer"
- "Code gradient descent from scratch"

---

## Resources Included

### For Each Topic:

1. **topic.md** (~200+ lines)
   - Theory and intuition
   - Mathematical derivations
   - Mermaid diagrams for visualization
   - Multiple examples
   - Common interview questions
   - Gotchas and misconceptions

2. **exercises.py**
   - Coding exercises
   - NumPy implementations from scratch
   - Unit tests
   - Expected outputs

3. **solutions.py**
   - Complete solutions with explanations
   - Best practices
   - Common mistakes to avoid

4. **notes.md** (template)
   - Your learning notes
   - Key insights
   - Things to review

### Reference Guides:

- **concepts-reference.md**: Quick formula lookup
- **foundations-guide.md**: Deep dive into calculus & statistics
- **GETTING_STARTED.md**: How to use this curriculum

---

## Getting Started

1. **Read the getting started guide**:
   ```bash
   cat GETTING_STARTED.md
   ```

2. **Review mathematical prerequisites** (if needed):
   ```bash
   cat foundations-guide.md
   ```

3. **Keep formulas handy**:
   ```bash
   cat concepts-reference.md
   ```

4. **Start with Topic 1**:
   ```bash
   cd 01-derivatives-gradients
   cat topic.md
   ```

5. **Work through exercises**:
   ```bash
   python exercises.py
   ```

6. **Document your learning**:
   - Fill in notes.md
   - Update your status in this README

---

## Success Tips

### Do's ✓
- Derive everything by hand at least once
- Implement concepts in NumPy before using frameworks
- Draw diagrams for complex concepts
- Practice explaining out loud
- Connect theory to practical applications

### Don'ts ✗
- Don't just memorize formulas
- Don't skip the mathematical derivations
- Don't rely on frameworks without understanding
- Don't move on if something is unclear
- Don't neglect the "why" for the "how"

---

## Timeline Expectations

- **Per Topic**: 3-4 hours (reading + exercises + implementation)
- **Phase 1**: 15-20 hours (1-2 weeks)
- **Phase 2**: 21-28 hours (2-3 weeks)
- **Phase 3**: 15-20 hours (1-2 weeks)
- **Phase 4**: 9-12 hours (1 week)

**Total**: 60-80 hours of focused study

---

## Interview Preparation Checklist

### 1 Week Before Interview
- [ ] Review all topic.md files
- [ ] Redo key derivations (backprop, cross-entropy gradient, etc.)
- [ ] Practice explaining core concepts out loud
- [ ] Review common interview questions from each topic

### 3 Days Before
- [ ] Review concepts-reference.md
- [ ] Go through all your notes.md files
- [ ] Practice whiteboard derivations
- [ ] Review your weak areas

### 1 Day Before
- [ ] Light review of key concepts
- [ ] Prepare questions to ask interviewer
- [ ] Get good rest

---

## Complementary with Coding Prep

This ML curriculum complements the coding interview prep:

- **Coding prep** teaches you algorithms and data structures
- **ML prep** teaches you the mathematics and concepts behind ML systems

For a complete DeepMind preparation, work through both curricula in parallel:
- Coding problems in mornings
- ML concepts in afternoons/evenings

---

**Ready to start? Head to [GETTING_STARTED.md](./GETTING_STARTED.md)!**

Good luck with your Google DeepMind ML interview preparation!
